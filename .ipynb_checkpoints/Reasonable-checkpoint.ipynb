{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c3ee18-fce5-4fd4-959b-69630a5a5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4926d475-b42e-44ce-9f7e-27a3fcc00bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = 'data/socal.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Folder where the images are stored\n",
    "image_folder = 'data/socal_pics'\n",
    "\n",
    "# Create a new column 'image_path' by constructing the path based on 'image_id'\n",
    "df['image_path'] = df['image_id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f334d9-1196-4184-8ac7-0c2dd3ad7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG16 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # VGG16 normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9759a0cb-afa4-44d9-9e1d-bfa8259ea86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained VGG16 model. Setting `pretrained=True` loads weights trained on ImageNet.\n",
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "# We only need the features, so we remove the classifier part by taking only `model.features`.\n",
    "model = model.features\n",
    "# Set the model to evaluation mode to prevent training-related behavior, such as dropout.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f4ad55-bd82-402f-b5d6-fa337e4d502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return np.zeros(4096)  # Return a zero vector if image is not found or corrupted\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb7a47e-fc7b-42b1-b079-3eca8ede3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image features\n",
    "image_features_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    features = extract_features(image_path)\n",
    "    image_features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d60745-9571-4aeb-b124-e44ba46f710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to numpy array\n",
    "image_features = np.array(image_features_list)\n",
    "\n",
    "# Save the image features to a numpy .npz file using np.savez\n",
    "np.savez('image_features.npz', image_features=image_features)\n",
    "\n",
    "# Prepare numerical features\n",
    "numerical_features = df[['sqft', 'n_citi', 'bed','bath']].values\n",
    "\n",
    "# Concatenate image features and numerical features\n",
    "X = np.concatenate((image_features, numerical_features), axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = df['price'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48eacd45-510f-4cd8-87d7-50c467992897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 8.67146494922878e+26\n",
      "Root Mean Squared Error (RMSE): 29447351237808.777\n"
     ]
    }
   ],
   "source": [
    "# Train linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ccceae-7ea3-4da9-903e-110f87151de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataset\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self,features,targets):\n",
    "        self.features = torch.tensor(features,dtype=torch.float32)\n",
    "        self.targets  = torch.tensor(targets,dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "    \n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetRegressor(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=64, output_size=2):\n",
    "        super(NeuralNetRegressor, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer -> Hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Hidden layer -> Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)    # Input to hidden\n",
    "        x = self.relu(x)   # ReLU activation\n",
    "        x = self.fc2(x)    # Hidden to output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411598dc-ce1f-4990-9a2e-68170d392116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, yhat):\n",
    "    return np.mean((y - yhat) ** 2)\n",
    "\n",
    "def RMSE(y,yhat):\n",
    "    return np.sqrt(MSE(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9809aba-c9cb-46be-8921-c49de8c14f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 561752240000.0\n",
      "epoch 480 loss 66656403000.0\n",
      "epoch 960 loss 95936370000.0\n",
      "epoch 1440 loss 114318660000.0\n",
      "epoch 1920 loss 63621456000.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RMSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m Yh \u001b[38;5;241m=\u001b[39m Yh\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;66;03m#GPT4 suggested change to add flatten to match shapes\u001b[39;00m\n\u001b[0;32m     50\u001b[0m Y \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m---> 51\u001b[0m \u001b[43mRMSE\u001b[49m(Y,Yh)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RMSE' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract numerical features\n",
    "numerical_features = df[['sqft', 'n_citi', 'bed', 'bath']].values  # Shape: (n_samples, 4)\n",
    "\n",
    "# Combine numerical features with image features\n",
    "features = np.hstack((numerical_features, image_features))  # Shape: (n_samples, total_feature_dim)\n",
    "\n",
    "# Extract targets\n",
    "targets = df.price\n",
    "\n",
    "# Create PyTorch dataset\n",
    "X = features\n",
    "Y = targets.to_numpy()\n",
    "dataset = CreateDataset(X,Y)\n",
    "data_loader = DataLoader(dataset,batch_size=200,shuffle=True)\n",
    "\n",
    "# Determine the input size based on the combined features\n",
    "input_size = features.shape[1]\n",
    "\n",
    "# Create the model instance\n",
    "model = NeuralNetRegressor(input_size=input_size,hidden_size=32,output_size=1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "cost_function = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 2400\n",
    "# Example of how to train the model (assuming you have your data loaders)\n",
    "for epoch in range(num_epochs):\n",
    "    for X,Y in data_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        Yh = model(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = cost_function(Yh,torch.unsqueeze(Y,1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 480 == 0:\n",
    "        print('epoch',epoch,'loss',loss.detach().numpy())\n",
    "        \n",
    "# neural network RMSE\n",
    "X = torch.tensor(features,dtype=torch.float32)\n",
    "Yh = model(X)\n",
    "Yh = Yh.detach().numpy().flatten() #GPT4 suggested change to add flatten to match shapes\n",
    "Y = targets.to_numpy()\n",
    "RMSE(Y,Yh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413afd1c-5348-4122-a64a-83b6b515ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch 0 loss 530075580000.0\n",
    "#epoch 480 loss 71383040000.0\n",
    "#epoch 960 loss 79980945000.0\n",
    "#epoch 1440 loss 81245790000.0\n",
    "#epoch 1920 loss 69038590000.0\n",
    "#278979.65244980756"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
