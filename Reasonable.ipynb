{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c3ee18-fce5-4fd4-959b-69630a5a5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torchvision.models import VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4926d475-b42e-44ce-9f7e-27a3fcc00bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = 'data/socal.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Folder where the images are stored\n",
    "image_folder = 'data/socal_pics'\n",
    "\n",
    "# Create a new column 'image_path' by constructing the path based on 'image_id'\n",
    "df['image_path'] = df['image_id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f334d9-1196-4184-8ac7-0c2dd3ad7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG16 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # VGG16 normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9759a0cb-afa4-44d9-9e1d-bfa8259ea86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained VGG16 model. Setting `pretrained=True` loads weights trained on ImageNet.\n",
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "# We only need the features, so we remove the classifier part by taking only `model.features`.\n",
    "model = model.features\n",
    "# Set the model to evaluation mode to prevent training-related behavior, such as dropout.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f4ad55-bd82-402f-b5d6-fa337e4d502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return np.zeros(4096)  # Return a zero vector if image is not found or corrupted\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7a47e-fc7b-42b1-b079-3eca8ede3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image features\n",
    "image_features_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    features = extract_features(image_path)\n",
    "    image_features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d60745-9571-4aeb-b124-e44ba46f710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to numpy array\n",
    "image_features = np.array(image_features_list)\n",
    "\n",
    "# Prepare numerical features\n",
    "numerical_features = df[['bed', 'bath', 'sqft']].values\n",
    "\n",
    "# Concatenate image features and numerical features\n",
    "X = np.concatenate((image_features, numerical_features), axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = df['price'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eacd45-510f-4cd8-87d7-50c467992897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
